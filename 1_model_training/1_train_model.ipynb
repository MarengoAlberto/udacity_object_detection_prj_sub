{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1cd147",
   "metadata": {},
   "source": [
    "# Tensorflow Object Detection API and AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85592c17",
   "metadata": {},
   "source": [
    "In this notebook, you will train and evaluate different models using the [Tensorflow Object Detection API](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/) and [AWS Sagemaker](https://aws.amazon.com/sagemaker/). \n",
    "\n",
    "If you ever feel stuck, you can refer to this [tutorial](https://aws.amazon.com/blogs/machine-learning/training-and-deploying-models-using-tensorflow-2-with-the-object-detection-api-on-amazon-sagemaker/).\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We are using the [Waymo Open Dataset](https://waymo.com/open/) for this project. The dataset has already been exported using the tfrecords format. The files have been created following the format described [here](https://tensorflow-object-detection-api-tutorial.readthedocs.io/en/latest/training.html#create-tensorflow-records). You can find data stored on [AWS S3](https://aws.amazon.com/s3/), AWS Object Storage. The images are saved with a resolution of 640x640."
   ]
  },
  {
   "cell_type": "code",
   "id": "dcc1d114",
   "metadata": {},
   "source": [
    "# %%capture\n",
    "# %pip install tensorflow_io sagemaker -U"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96f55350",
   "metadata": {},
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.estimator import Estimator\n",
    "from framework import CustomFramework"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ccde6fd1",
   "metadata": {},
   "source": [
    "Save the IAM role in a variable called `role`. This would be useful when training the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "0ab6b13b",
   "metadata": {},
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "print(role)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ae64e29f",
   "metadata": {},
   "source": [
    "# The train and val paths below are public S3 buckets created by Udacity for this project\n",
    "inputs = {'train': 's3://cd2688-object-detection-tf2/train/', \n",
    "          'val': 's3://cd2688-object-detection-tf2/val/'} \n",
    "\n",
    "# Insert path of a folder in your personal S3 bucket to store tensorboard logs.\n",
    "tensorboard_s3_prefix = 's3://object-detection-prj/logs/'"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fc16a825",
   "metadata": {},
   "source": [
    "## Container\n",
    "\n",
    "To train the model, you will first need to build a [docker](https://www.docker.com/) container with all the dependencies required by the TF Object Detection API. The code below does the following:\n",
    "* clone the Tensorflow models repository\n",
    "* get the exporter and training scripts from the repository\n",
    "* build the docker image and push it \n",
    "* print the container name"
   ]
  },
  {
   "cell_type": "code",
   "id": "1ad5ac8c",
   "metadata": {},
   "source": [
    "%%bash\n",
    "\n",
    "# # clone the repo and get the scripts\n",
    "# git clone https://github.com/tensorflow/models.git docker/models\n",
    "\n",
    "# # get model_main and exporter_main files from TF2 Object Detection GitHub repository\n",
    "# cp docker/models/research/object_detection/exporter_main_v2.py source_dir \n",
    "# cp docker/models/research/object_detection/model_main_tf2.py source_dir"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2dab3f0",
   "metadata": {},
   "source": [
    "# # build and push the docker image. This code can be commented out after being run once.\n",
    "# # This will take around 10 mins.\n",
    "# image_name = 'tf2-object-detection'\n",
    "# !sh ./docker/build_and_push.sh $image_name"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e62b3562",
   "metadata": {},
   "source": [
    "To verify that the image was correctly pushed to the [Elastic Container Registry](https://aws.amazon.com/ecr/), you can look at it in the AWS webapp. For example, below you can see that three different images have been pushed to ECR. You should only see one, called `tf2-object-detection`.\n",
    "![ECR Example](../data/example_ecr.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "0310b6a5",
   "metadata": {},
   "source": [
    "# display the container name\n",
    "with open (os.path.join('docker', 'ecr_image_fullname.txt'), 'r') as f:\n",
    "    container = f.readlines()[0][:-1]\n",
    "\n",
    "print(container)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13b2a754",
   "metadata": {},
   "source": [
    "## Pre-trained model from model zoo\n",
    "\n",
    "As often, we are not training from scratch and we will be using a pretrained model from the TF Object Detection model zoo. You can find pretrained checkpoints [here](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). Because your time is limited for this project, we recommend to only experiment with the following models:\n",
    "* SSD MobileNet V2 FPNLite 640x640\t\n",
    "* SSD ResNet50 V1 FPN 640x640 (RetinaNet50)\t\n",
    "* Faster R-CNN ResNet50 V1 640x640\t\n",
    "* EfficientDet D1 640x640\t\n",
    "* Faster R-CNN ResNet152 V1 640x640\t\n",
    "\n",
    "In the code below, the EfficientDet D1 model is downloaded and extracted. This code should be adjusted if you were to experiment with other architectures."
   ]
  },
  {
   "cell_type": "code",
   "id": "13e5881a",
   "metadata": {},
   "source": [
    "%%bash\n",
    "mkdir /tmp/checkpoint\n",
    "mkdir source_dir/checkpoint\n",
    "wget -O /tmp/fasterrcnn152.tar.gz http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet152_v1_640x640_coco17_tpu-8.tar.gz\n",
    "tar -zxvf /tmp/fasterrcnn152.tar.gz --strip-components 2 --directory source_dir/checkpoint faster_rcnn_resnet152_v1_640x640_coco17_tpu-8/checkpoint\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eca243d9",
   "metadata": {},
   "source": [
    "# %%bash\n",
    "# mkdir /tmp/checkpoint\n",
    "# mkdir source_dir/checkpoint\n",
    "# wget -O /tmp/fasterrcnn.tar.gz http://download.tensorflow.org/models/object_detection/tf2/20200711/faster_rcnn_resnet101_v1_640x640_coco17_tpu-8.tar.gz\n",
    "# tar -zxvf /tmp/fasterrcnn.tar.gz --strip-components 2 --directory source_dir/checkpoint faster_rcnn_resnet101_v1_640x640_coco17_tpu-8/checkpoint"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8c4b1d46",
   "metadata": {},
   "source": [
    "# %%bash\n",
    "# mkdir /tmp/checkpoint\n",
    "# mkdir source_dir/checkpoint\n",
    "# wget -O /tmp/efficientdet.tar.gz http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d1_coco17_tpu-32.tar.gz\n",
    "# tar -zxvf /tmp/efficientdet.tar.gz --strip-components 2 --directory source_dir/checkpoint efficientdet_d1_coco17_tpu-32/checkpoint"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a1e04a98",
   "metadata": {},
   "source": [
    "## Edit pipeline.config file\n",
    "\n",
    "The [`pipeline.config`](source_dir/pipeline.config) in the `source_dir` folder should be updated when you experiment with different models. The different config files are available [here](https://github.com/tensorflow/models/tree/master/research/object_detection/configs/tf2).\n",
    "\n",
    ">Note: The provided `pipeline.config` file works well with the `EfficientDet` model. You would need to modify it when working with other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47483545",
   "metadata": {},
   "source": [
    "## Launch Training Job\n",
    "\n",
    "Now that we have a dataset, a docker image and some pretrained model weights, we can launch the training job. To do so, we create a [Sagemaker Framework](https://sagemaker.readthedocs.io/en/stable/frameworks/index.html), where we indicate the container name, name of the config file, number of training steps etc.\n",
    "\n",
    "The `run_training.sh` script does the following:\n",
    "* train the model for `num_train_steps` \n",
    "* evaluate over the val dataset\n",
    "* export the model\n",
    "\n",
    "Different metrics will be displayed during the evaluation phase, including the mean average precision. These metrics can be used to quantify your model performances and compare over the different iterations.\n",
    "\n",
    "You can also monitor the training progress by navigating to **Training -> Training Jobs** from the Amazon Sagemaker dashboard in the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "id": "0c7175cc",
   "metadata": {},
   "source": [
    "tensorboard_output_config = sagemaker.debugger.TensorBoardOutputConfig(\n",
    "    s3_output_path=tensorboard_s3_prefix,\n",
    "    container_local_output_path='/opt/training/'\n",
    ")\n",
    "\n",
    "estimator = CustomFramework(\n",
    "    role=role,\n",
    "    image_uri=container,\n",
    "    entry_point='run_training.sh',\n",
    "    source_dir='source_dir/',\n",
    "    hyperparameters={\n",
    "        \"model_dir\": \"/opt/training\",        \n",
    "        \"pipeline_config_path\": \"pipeline.config\",\n",
    "        \"num_train_steps\": \"4000\",    \n",
    "        \"sample_1_of_n_eval_examples\": \"1\"\n",
    "    },\n",
    "    instance_count=1,\n",
    "    instance_type='ml.g5.xlarge',\n",
    "    tensorboard_output_config=tensorboard_output_config,\n",
    "    disable_profiler=True,\n",
    "    base_job_name='tf2-object-detection'\n",
    ")\n",
    "\n",
    "estimator.fit(inputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "84545881",
   "metadata": {},
   "source": [
    "You should be able to see your model training in the AWS webapp as shown below:\n",
    "![ECR Example](../data/example_trainings.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9844f25",
   "metadata": {},
   "source": [
    "## Improve on the initial model\n",
    "\n",
    "Most likely, this initial experiment did not yield optimal results. However, you can make multiple changes to the `pipeline.config` file to improve this model. One obvious change consists in improving the data augmentation strategy. The [`preprocessor.proto`](https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto) file contains the different data augmentation method available in the Tf Object Detection API. Justify your choices of augmentations in the write-up.\n",
    "\n",
    "Keep in mind that the following are also available:\n",
    "* experiment with the optimizer: type of optimizer, learning rate, scheduler etc\n",
    "* experiment with the architecture. The Tf Object Detection API model zoo offers many architectures. Keep in mind that the pipeline.config file is unique for each architecture and you will have to edit it.\n",
    "* visualize results on the test frames using the `2_deploy_model` notebook available in this repository.\n",
    "\n",
    "In the cell below, write down all the different approaches you have experimented with, why you have chosen them and what you would have done if you had more time and resources. Justify your choices using the tensorboard visualizations (take screenshots and insert them in your write-up), the metrics on the evaluation set and the generated animation you have created with [this tool](../2_run_inference/2_deploy_model.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067059d0",
   "metadata": {},
   "source": [
    "# Experiment Results\n",
    "\n",
    "## Run 1 - Baseline - EfficientDet D1 640x640\n",
    "\n",
    "```python\n",
    "DONE (t=0.21s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.080\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.187\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.058\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.030\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.334\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.292\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.022\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.095\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.130\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.068\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.435\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.608\n",
    "```\n",
    "\n",
    "## Run 2\n",
    "\n",
    "- SSD EfficientDet D1 640x640\n",
    "- num_train_steps: 4000\n",
    "- lowered lr\n",
    "    ```\n",
    "      optimizer {\n",
    "    momentum_optimizer {\n",
    "      learning_rate {\n",
    "        cosine_decay_learning_rate {\n",
    "          learning_rate_base: 0.007999999821186066\n",
    "          total_steps: 4000\n",
    "          warmup_learning_rate: 0.0010000000474974513\n",
    "          warmup_steps: 150\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.8999999761581421\n",
    "    }\n",
    "    use_moving_average: false\n",
    "  }\n",
    "    ```\n",
    "- Added Augmentations in train config:\n",
    "\n",
    "    ```\n",
    "    data_augmentation_options {\n",
    "        random_adjust_brightness {\n",
    "        }\n",
    "    }\n",
    "      data_augmentation_options {\n",
    "        random_patch_gaussian {\n",
    "        }\n",
    "      }\n",
    "      data_augmentation_options {\n",
    "          random_image_scale {\n",
    "        }\n",
    "      }\n",
    "      data_augmentation_options {\n",
    "        random_adjust_contrast {\n",
    "        }\n",
    "      }\n",
    "      data_augmentation_options {\n",
    "        random_adjust_saturation {\n",
    "        }\n",
    "      }\n",
    "    ```\n",
    "    \n",
    "```python\n",
    "DONE (t=0.21s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.266\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.093\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.051\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.409\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.505\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.028\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.120\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.169\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.103\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.506\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.617\n",
    "```\n",
    "\n",
    "## Run 3\n",
    "\n",
    "- Faster R-CNN ResNet101 V1 640x640\n",
    "- num_train_steps: 4000\n",
    "- Same lr as run 2\n",
    "- Same augmentations as run 2\n",
    "\n",
    "```python\n",
    "DONE (t=0.37s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.155\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.311\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.135\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.078\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.502\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.785\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.034\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.149\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.205\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.134\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.567\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.882\n",
    "```\n",
    "\n",
    "## Run 4\n",
    "\n",
    "- Faster R-CNN ResNet152 V1 640x640\n",
    "- num_train_steps: 4000\n",
    "- lr as per model training config adjusted to fine-tuning steps\n",
    "```\n",
    "    optimizer {\n",
    "        momentum_optimizer {\n",
    "          learning_rate {\n",
    "            cosine_decay_learning_rate {\n",
    "              learning_rate_base: .04\n",
    "              total_steps: 4000\n",
    "              warmup_learning_rate: .013333\n",
    "              warmup_steps: 150\n",
    "            }\n",
    "          }\n",
    "          momentum_optimizer_value: 0.8999999761581421\n",
    "        }\n",
    "        use_moving_average: false\n",
    "    }\n",
    "   ```\n",
    "- Same augmentations as run 2\n",
    "\n",
    "```python\n",
    "DONE (t=0.36s).\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.122\n",
    " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.245\n",
    " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.107\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.053\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.411\n",
    " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.675\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.029\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.122\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.170\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.103\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.497\n",
    " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.712\n",
    "```\n",
    "\n",
    "## In summary\n",
    "\n",
    "**Baseline (EfficientDet D1 640x640):**\n",
    "The initial run with the baseline EfficientDet D1 model achieved a mean average precision (mAP) of about 0.080 (IoU=0.50:0.95). While this was a reasonable starting point, a closer look at object scale metrics indicated where the model struggled:\n",
    "\n",
    "Small objects: AP ~0.030\n",
    "The model found it challenging to detect small objects reliably, likely due to its feature pyramid design not sufficiently capturing fine-grained details at the smallest scales or insufficient training steps.\n",
    "\n",
    "Medium objects: AP ~0.334\n",
    "Medium-sized objects fared better as the model’s multi-scale features are better aligned with the visual granularity of these objects.\n",
    "\n",
    "Large objects: AP ~0.292\n",
    "Large objects were somewhat easier to detect than small ones but did not reach the performance levels one might expect. This could be due to dataset characteristics or suboptimal tuning of hyperparameters that govern receptive fields and scaling.\n",
    "\n",
    "**EfficientDet D1 with Augmentations and Adjusted Learning Rate:**\n",
    "Introducing targeted data augmentations (random brightness, contrast, saturation, scaling, and Gaussian noise patches) along with a reduced learning rate led to an improved mAP of about 0.118. Looking at the scales:\n",
    "\n",
    "Small objects: AP ~0.051\n",
    "Augmentations and a more stable learning rate helped the model learn more robust features, improving detection of small objects.\n",
    "\n",
    "Medium objects: AP ~0.409\n",
    "Medium-sized objects benefited substantially. Data augmentation likely helped the model generalize better to varying environmental conditions and slight scale differences.\n",
    "\n",
    "Large objects: AP ~0.505\n",
    "The largest gain was seen in large object detection. The introduction of scaling augmentations possibly helped the model adjust its receptive fields more effectively, while color augmentations (contrast, brightness, saturation) improved model robustness to diverse lighting conditions.\n",
    "\n",
    "**Why these Augmentations?**\n",
    "\n",
    "Random Brightness/Contrast/Saturation: Real-world scenes vary greatly in lighting and color conditions. Adjusting brightness, contrast, and saturation forces the model to become invariant to these variations, reducing overfitting to a particular lighting scenario.\n",
    "Random Image Scaling: Scaling teaches the model to handle objects at different sizes, improving its ability to detect both small and large objects in the real-world scenarios.\n",
    "Gaussian Patch Noise: Introducing local noise encourages the model to rely on structural patterns rather than clean pixels. This can improve robustness against sensor noise and compression artifacts often found in automotive imagery.\n",
    "\n",
    "**Faster R-CNN ResNet101 V1 640x640:**\n",
    "Switching to a different architecture known for more robust feature extraction and region proposal methods yielded an mAP of about 0.155.\n",
    "\n",
    "Small objects: AP ~0.078\n",
    "The jump in small-object AP suggests that the more powerful backbone (ResNet101) combined with the region proposal mechanism of Faster R-CNN helps isolate smaller objects more effectively than the single-shot approach of EfficientDet.\n",
    "\n",
    "Medium objects: AP ~0.502\n",
    "Medium-sized objects again show a strong improvement. The deeper ResNet101 backbone offers richer feature representations that help differentiate medium-scale objects more clearly.\n",
    "\n",
    "Large objects: AP ~0.785\n",
    "Large-object detection experienced a dramatic improvement. This underscores the advantage of a two-stage detector like Faster R-CNN for objects that span a larger fraction of the image, as region proposals and subsequent refinement steps can more accurately localize bigger targets.\n",
    "\n",
    "**Faster R-CNN ResNet152 V1 640x640:**\n",
    "While we might expect a deeper backbone to improve results further, this run yielded an mAP of about 0.122, slightly lower than the ResNet101 run.\n",
    "\n",
    "Small objects: AP ~0.053\n",
    "Performance on small objects dipped slightly compared to ResNet101. This could be due to suboptimal hyperparameters or training steps, as deeper networks sometimes require more careful tuning or longer training.\n",
    "\n",
    "Medium objects: AP ~0.411\n",
    "Medium-object performance remained decent but did not match the gains seen with ResNet101.\n",
    "\n",
    "Large objects: AP ~0.675\n",
    "Although still strong, large-object detection did not reach the heights achieved by ResNet101. This suggests that simply increasing depth does not guarantee better performance without corresponding adjustments in optimization and augmentation strategies.\n",
    "\n",
    "\n",
    "Here is the screenshot from Tensorboard showing the mAP (large, medium and small) for the different runs:\n",
    "\n",
    "![alt text](image.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "id": "d18e165e",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p310",
   "language": "python",
   "name": "conda_tensorflow2_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
